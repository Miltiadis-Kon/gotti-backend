{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set up the mongo client where the hastags and output tweets are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# MongoDB setup\n",
    "client = MongoClient('mongodb://localhost:27017/')  # Replace with your MongoDB connection string\n",
    "db = client['twitter_db']\n",
    "collection = db['tweets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Scrape hashags using snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_tweets(hashtag, limit=100):\n",
    "    query = f\"#{hashtag}\"\n",
    "    tweets = []\n",
    "\n",
    "    for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "        if len(tweets) >= limit:\n",
    "            break\n",
    "        \n",
    "        tweet_data = {\n",
    "            'date': tweet.date,\n",
    "            'id': tweet.id,\n",
    "            'content': tweet.content,\n",
    "            'user': tweet.user.username,\n",
    "            'hashtags': [tag.lower() for tag in tweet.hashtags],\n",
    "            'likes': tweet.likeCount,\n",
    "            'retweets': tweet.retweetCount\n",
    "        }\n",
    "        tweets.append(tweet_data)\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "hashtag = \"python\"  # Replace with your desired hashtag\n",
    "limit = 100  # Number of tweets to scrape\n",
    "\n",
    "print(f\"Scraping tweets for #{hashtag}...\")\n",
    "tweets = scrape_tweets(hashtag, limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Scrape using ntscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ntscraper\n",
      "  Downloading ntscraper-0.3.16-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from ntscraper) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from ntscraper) (4.12.3)\n",
      "Requirement already satisfied: lxml>=4.9 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from ntscraper) (5.3.0)\n",
      "Collecting tqdm>=4.66 (from ntscraper)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from beautifulsoup4>=4.11->ntscraper) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from requests>=2.28->ntscraper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from requests>=2.28->ntscraper) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from requests>=2.28->ntscraper) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from requests>=2.28->ntscraper) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from tqdm>=4.66->ntscraper) (0.4.6)\n",
      "Downloading ntscraper-0.3.16-py3-none-any.whl (12 kB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, ntscraper\n",
      "Successfully installed ntscraper-0.3.16 tqdm-4.66.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ntscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing instances: 100%|██████████| 77/77 [01:36<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16-Aug-24 20:18:25 - No instance specified, using random instance https://nitter.privacydev.net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16-Aug-24 20:18:25 - Empty page on https://nitter.privacydev.net\n",
      "{'tweets': [], 'threads': []}\n"
     ]
    }
   ],
   "source": [
    "from ntscraper import Nitter\n",
    "\n",
    "scraper = Nitter(log_level=1, skip_instance_check=False)\n",
    "\n",
    "terms = [\"AAPL\"]\n",
    "\n",
    "tweets = scraper.get_tweets(terms, mode='hashtag', number=2)\n",
    "\n",
    "print(tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Twint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting twint\n",
      "  Downloading twint-2.1.20.tar.gz (31 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting aiohttp (from twint)\n",
      "  Downloading aiohttp-3.10.3-cp311-cp311-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting aiodns (from twint)\n",
      "  Downloading aiodns-3.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from twint) (4.12.3)\n",
      "Collecting cchardet (from twint)\n",
      "  Downloading cchardet-2.1.7.tar.gz (653 kB)\n",
      "     ---------------------------------------- 0.0/653.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/653.6 kB ? eta -:--:--\n",
      "     ---------------- ----------------------- 262.1/653.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 653.6/653.6 kB 1.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting elasticsearch (from twint)\n",
      "  Downloading elasticsearch-8.15.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: pysocks in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from twint) (1.7.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from twint) (2.2.2)\n",
      "Collecting aiohttp_socks (from twint)\n",
      "  Downloading aiohttp_socks-0.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting schedule (from twint)\n",
      "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting geopy (from twint)\n",
      "  Downloading geopy-2.4.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting fake-useragent (from twint)\n",
      "  Downloading fake_useragent-1.5.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting googletransx (from twint)\n",
      "  Downloading googletransx-2.4.2.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pycares>=4.0.0 (from aiodns->twint)\n",
      "  Downloading pycares-4.4.0-cp311-cp311-win_amd64.whl.metadata (4.5 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->twint)\n",
      "  Downloading aiohappyeyeballs-2.3.6-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->twint)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->twint)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->twint)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->twint)\n",
      "  Downloading multidict-6.0.5-cp311-cp311-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->twint)\n",
      "  Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl.metadata (32 kB)\n",
      "Collecting python-socks<3.0.0,>=2.4.3 (from python-socks[asyncio]<3.0.0,>=2.4.3->aiohttp_socks->twint)\n",
      "  Downloading python_socks-2.5.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from beautifulsoup4->twint) (2.6)\n",
      "Collecting elastic-transport<9,>=8.13 (from elasticsearch->twint)\n",
      "  Downloading elastic_transport-8.15.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting geographiclib<3,>=1.52 (from geopy->twint)\n",
      "  Downloading geographiclib-2.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from googletransx->twint) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from pandas->twint) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from pandas->twint) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from pandas->twint) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from pandas->twint) (2024.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from elastic-transport<9,>=8.13->elasticsearch->twint) (2.2.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from elastic-transport<9,>=8.13->elasticsearch->twint) (2024.7.4)\n",
      "Requirement already satisfied: cffi>=1.5.0 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from pycares>=4.0.0->aiodns->twint) (1.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->twint) (1.16.0)\n",
      "Collecting async-timeout>=3.0.1 (from python-socks[asyncio]<3.0.0,>=2.4.3->aiohttp_socks->twint)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp->twint) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from requests->googletransx->twint) (3.3.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\m\\desktop\\repos\\gotti\\gotti-backend\\.conda\\lib\\site-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns->twint) (2.22)\n",
      "Downloading aiodns-3.2.0-py3-none-any.whl (5.7 kB)\n",
      "Downloading aiohttp-3.10.3-cp311-cp311-win_amd64.whl (378 kB)\n",
      "Downloading aiohttp_socks-0.9.0-py3-none-any.whl (9.7 kB)\n",
      "Downloading elasticsearch-8.15.0-py3-none-any.whl (523 kB)\n",
      "Downloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
      "Downloading geopy-2.4.1-py3-none-any.whl (125 kB)\n",
      "Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
      "Downloading aiohappyeyeballs-2.3.6-py3-none-any.whl (12 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Downloading elastic_transport-8.15.0-py3-none-any.whl (64 kB)\n",
      "Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\n",
      "Downloading multidict-6.0.5-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Downloading pycares-4.4.0-cp311-cp311-win_amd64.whl (76 kB)\n",
      "Downloading python_socks-2.5.0-py3-none-any.whl (52 kB)\n",
      "Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl (76 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Building wheels for collected packages: twint, cchardet, googletransx\n",
      "  Building wheel for twint (setup.py): started\n",
      "  Building wheel for twint (setup.py): finished with status 'done'\n",
      "  Created wheel for twint: filename=twint-2.1.20-py3-none-any.whl size=33945 sha256=79a781ea09b497fdd9c711b1112f65719f8ee551cc6bc14b39cf137830db3175\n",
      "  Stored in directory: c:\\users\\m\\appdata\\local\\pip\\cache\\wheels\\e1\\0d\\0e\\1384ada6f5ca0ec87cd8d48d107d69b782fb76b3a138fc3f8a\n",
      "  Building wheel for cchardet (setup.py): started\n",
      "  Building wheel for cchardet (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for cchardet\n",
      "  Building wheel for googletransx (setup.py): started\n",
      "  Building wheel for googletransx (setup.py): finished with status 'done'\n",
      "  Created wheel for googletransx: filename=googletransx-2.4.2-py3-none-any.whl size=16020 sha256=2567a3d1c9d11a51c897ef3a32902b32ef11d9a8a362797b3e2e5cc8784d0fba\n",
      "  Stored in directory: c:\\users\\m\\appdata\\local\\pip\\cache\\wheels\\b1\\35\\1b\\145e2fa3229232fce5e9de984e82843abf7c1a27cec582398e\n",
      "Successfully built twint googletransx\n",
      "Failed to build cchardet\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [11 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\cchardet\n",
      "      copying src\\cchardet\\version.py -> build\\lib.win-amd64-cpython-311\\cchardet\n",
      "      copying src\\cchardet\\__init__.py -> build\\lib.win-amd64-cpython-311\\cchardet\n",
      "      running build_ext\n",
      "      building 'cchardet._cchardet' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for cchardet\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (cchardet)\n"
     ]
    }
   ],
   "source": [
    "pip install twint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Store in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def store_in_mongodb(tweets):\n",
    "    try:\n",
    "        result = collection.insert_many(tweets)\n",
    "        print(f\"Inserted {len(result.inserted_ids)} documents into MongoDB\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while storing tweets: {e}\")\n",
    "\n",
    "if tweets:\n",
    "    print(f\"Scraped {len(tweets)} tweets. Storing in MongoDB...\")\n",
    "    store_in_mongodb(tweets)\n",
    "else:\n",
    "    print(\"No tweets found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
